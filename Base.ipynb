{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをベースに実験をしていこう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "### 1. [Utils ツール](#utils)\n",
    "### 2. [Read data データの読み込み](#read)\n",
    "### 3. [Preprocessers 前処理](#preprocess)\n",
    "### 4. [Features 特徴抽出](#features)\n",
    "### 5. [Metrics 評価指標](#metrics)\n",
    "### 6. [Classifiers 分類器](#classifiers)\n",
    "### 7. [Integration 統合](#integration)\n",
    "### 8. [result 結果](#result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#訓練データの正解データ\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basepath = \"../../first_retailing/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"utils\">Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "#画像のpixelを作成\n",
    "def get_img_array(path,resize):\n",
    "    from PIL import ImageOps\n",
    "    img_array = np.asarray(ImageOps.grayscale(Image.open(path)).resize([resize,resize]))#.getdata()\n",
    "    return img_array\n",
    "\n",
    "#画像を表示\n",
    "def show_img(path):\n",
    "    img_array = Image.open(path).resize([128,128])\n",
    "    img_array.show()\n",
    "\n",
    "#精度\n",
    "def M_accuracy(true, pred):\n",
    "    M_acc = {}\n",
    "    for i in range(24):\n",
    "        y_id = pred[true[\"category_id\"].values==i] == i\n",
    "        M_acc[i]=np.mean(y_id)\n",
    "    return  np.mean([j for j in M_acc.values()])\n",
    "\n",
    "\n",
    "#pickle書き出し(<4GB)\n",
    "import pickle\n",
    "def write_pickle(pickle_path,v):\n",
    "    with open(pickle_path, mode='wb') as f:\n",
    "        pickle.dump(v, f)\n",
    "    print(\"Finished Writing pickle\")\n",
    "\n",
    "#pickle 読み込み \n",
    "def read_pickle(pickle_path):\n",
    "    with open(pickle_path, mode='rb') as f:\n",
    "        read = pickle.load(f)\n",
    "    return read\n",
    "\n",
    "def show_acc(clf):\n",
    "    y_train_pred =  clf.predict(X_train)\n",
    "    y_test_pred =  clf.predict(X_test)\n",
    "    train_accuracy = M_accuracy(true=y_train, pred=y_train_pred)\n",
    "    test_accuracy  = M_accuracy(true=y_test, pred=y_test_pred)\n",
    "    print(\"Train Accuracy: %s\\n Test Accuracy: %s\"%(train_accuracy, test_accuracy))\n",
    "    \n",
    "#kerasのhistoryをプロット\n",
    "def plot_history(history):\n",
    "    # 精度の履歴をプロット\n",
    "    plt.plot(history.history['acc'],\"o-\",label=\"accuracy\")\n",
    "    plt.plot(history.history['val_acc'],\"o-\",label=\"val_acc\")\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # 損失の履歴をプロット\n",
    "    plt.plot(history.history['loss'],\"o-\",label=\"loss\",)\n",
    "    plt.plot(history.history['val_loss'],\"o-\",label=\"val_loss\")\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    # modelに学習させた時の変化の様子をplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"read\">Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = pd.read_csv(basepath+\"train_master.tsv\",sep=\"\\t\")\n",
    "X = np.load(\"../X_20.npy\")\n",
    "test = np.load(\"../test_20.npy\") #test_0 ~ test_len()の順番\n",
    "\n",
    "resize = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Chang/.pyenv/versions/python351/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test= train_test_split(X, y ,test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"preprocess\">Preprocessers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, IncrementalPCA,KernelPCA,FactorAnalysis,FastICA,TruncatedSVD,NMF\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "\n",
    "#すべての画像のピクセルごとの平均値を、それぞれの画像から引く\n",
    "def pp_mean_subtraction_per_pixel(X_train, X_test):\n",
    "    \"\"\"\n",
    "    input: array with shape(row,resize,resize,3)\n",
    "    output: shape(row,resize,resize,3), shape(row,resize,resize,3)\n",
    "    \"\"\"\n",
    "    X_train_after = np.zeros(X_train.shape)\n",
    "    X_test_after  = np.zeros(X_test.shape)\n",
    "    shape = X_train.shape\n",
    "    for f in range(shape[1]):\n",
    "        for s in range(shape[2]):\n",
    "            for t in range(shape[3]):\n",
    "                mean = np.mean(X_train[:,f,s,t])\n",
    "                X_train_after[:,f,s,t] = X_train[:,f,s,t] - mean\n",
    "                X_test_after[:,f,s,t]  = X_test[:,f,s,t] - mean\n",
    "    return X_train_after, X_test_after\n",
    "\n",
    "\n",
    "#すべての画像のピクセルごとに正規化\n",
    "def pp_normalization_per_pixel(X_train, X_test):\n",
    "    \"\"\"\n",
    "    input: array with shape(row,resize,resize,3)\n",
    "    output: same shape array\n",
    "    \"\"\"\n",
    "    X_train_after = np.zeros(X_train.shape)\n",
    "    X_test_after  = np.zeros(X_test.shape)\n",
    "    shape = X_train.shape\n",
    "    for f in range(shape[1]):\n",
    "        for s in range(shape[2]):\n",
    "            for t in range(shape[3]):\n",
    "                mean = np.mean(X_train[:,f,s,t])\n",
    "                std  = np.std(X_train[:,f,s,t])\n",
    "                X_train_after[:,f,s,t] = (X_train[:,f,s,t] - mean)/std\n",
    "                X_test_after[:,f,s,t]  = (X_test[:,f,s,t] - mean)/std\n",
    "    return X_train_after, X_test_after\n",
    "\n",
    "\n",
    "#画像サイズを削減\n",
    "def pp_random_crop(X_train,X_test,size):\n",
    "    \"\"\"\n",
    "    input: array with shape(row,resize,resize,3)\n",
    "    output: array with shape(row,size,size,3)\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    first = np.sort(np.random.choice(X_train.shape[1], size, replace=False))\n",
    "    X_train_after,X_test_after = X_train[:,first,:,:], X_test[:,first,:,:]\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    second = np.sort(np.random.choice(X_train.shape[2], size, replace=False))   \n",
    "    X_train_after,X_test_after = X_train_after[:,:,second,:], X_test_after[:,:,second,:]\n",
    "    return X_train_after,X_test_after\n",
    "\n",
    "#次元削除\n",
    "def pp_decompotion(types, X_train, X_test, n_comp):\n",
    "    decomposer = {\"pca\":PCA, \"ipca\":IncrementalPCA, \"kpca\":KernelPCA, \"fa\":FactorAnalysis, \n",
    "                  \"fica\":FastICA, \"svd\":TruncatedSVD, \"nmf\":NMF, \"tsne\":TSNE, \"mds\":MDS}\n",
    "    try:\n",
    "        dec = decomposer[types]\n",
    "        dec = dec(n_components=n_comp)\n",
    "        X_train_dec = dec.fit_transform(X_train)\n",
    "        X_test_dec = dec.transform(X_test)    \n",
    "        return X_train_dec, X_test_dec    \n",
    "    \n",
    "    except:\n",
    "        print(\"そんなのありません\")\n",
    "        print([k for k in decomposer.key])\n",
    "        \n",
    "#resample\n",
    "#パラメータを変更しても良い http://contrib.scikit-learn.org/imbalanced-learn/api.html\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "def over_sampling(types, X_train, y_train):\n",
    "    RANDOM_STATE = 0\n",
    "    samplers = {'ADASYN':ADASYN(random_state=RANDOM_STATE),\n",
    "                'ROS':RandomOverSampler(random_state=RANDOM_STATE),\n",
    "                'SMOTE':SMOTE(random_state=RANDOM_STATE)}\n",
    "    method = samplers[types]\n",
    "    X_resampled, y_resampled = method.fit_sample(X_train, y_train)\n",
    "    \n",
    "    return  X_resampled, y_resampled\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour,AllKNN, EditedNearestNeighbours,RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import NearMiss, NeighbourhoodCleaningRule, OneSidedSelection, TomekLinks\n",
    "def under_sampling(types, X_train, y_train):\n",
    "    RANDOM_STATE = 0\n",
    "    samplers = {'CNN':CondensedNearestNeighbour(random_state=RANDOM_STATE),\n",
    "                'ENN':EditedNearestNeighbours(random_state=RANDOM_STATE),\n",
    "                'AKNN':AllKNN(random_state=RANDOM_STATE),                \n",
    "                'RENN':RepeatedEditedNearestNeighbours(random_state=RANDOM_STATE),   \n",
    "                'NM':NearMiss(random_state=RANDOM_STATE),   \n",
    "                'NCR':NeighbourhoodCleaningRule(random_state=RANDOM_STATE),   \n",
    "                'OSS':OneSidedSelection(random_state=RANDOM_STATE),   \n",
    "                'TL':TomekLinks(random_state=RANDOM_STATE),                   \n",
    "               }\n",
    "    method = samplers[types]\n",
    "    X_resampled, y_resampled = method.fit_sample(X_train, y_train)\n",
    "    \n",
    "    return  X_resampled, y_resampled    \n",
    "    \n",
    "    \n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "def hybrid_sampling(types, X_train, y_train):\n",
    "    RANDOM_STATE = 0\n",
    "    samplers = {'SMOTEENN':SMOTEENN(random_state=RANDOM_STATE),\n",
    "                'SMOTETomek':SMOTETomek(random_state=RANDOM_STATE)}\n",
    "    method = samplers[types]\n",
    "    X_resampled, y_resampled = method.fit_sample(X_train, y_train)\n",
    "    \n",
    "    return  X_resampled, y_resampled\n",
    "\n",
    "#カテゴリか否かの2値に変換\n",
    "def to_binary(y=y, target=0):\n",
    "    return pd.DataFrame([1 if value==target else 0 for value in y[\"category_id\"].values], index=y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ピクセル位置ごとに平均値除去\n",
    "X_train_pp, X_test_pp = pp_mean_subtraction_per_pixel(X_train,X_test)\n",
    "\n",
    "\n",
    "#ピクセル位置ごとに正規化\n",
    "#X_train_pp, X_test_pp = pp_normalization_per_pixel(X_train,X_test)\n",
    "\n",
    "\n",
    "#ランダムにsize*sizeになるまで除去\n",
    "X_train_pp,X_test_pp = pp_random_crop(X_train_pp, X_test_pp, size=18)\n",
    "\n",
    "\n",
    "#普通の次元削除(faが良さそう)\n",
    "#X_train_pp, X_test_pp = pp_decompotion(types=\"fa\",X_train,X_test,n_comp=10)\n",
    "\n",
    "#ラベル1か否かの変換\n",
    "#to_binary(y_train, target=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_pp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#oversample: 'ADASYN', 'ROS', 'SMOTE'\n",
    "#X_train_resampled, y_train_resampled = over_sampling('ADASYN', X_train, y_train[\"category_id\"])\n",
    "\n",
    "#undersample: 'CNN', 'ENN', 'AKNN', 'RENN', 'NM', 'NCR', 'OSS', 'TL'\n",
    "#X_train_resampled, y_train_resampled = under_sampling('ENN', X_train, y_train[\"category_id\"])\n",
    "\n",
    "#hybridsample: 'SMOTEENN', 'SMOTETomek'\n",
    "#X_train_resampled, y_train_resampled = hybrid_sampling(\"SMOTEENN\", X_train, y_train[\"category_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"features\">Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#サブモジュール\n",
    "from scipy import stats\n",
    "\n",
    "#最小値\n",
    "def p0(array):\n",
    "    return stats.scoreatpercentile(array, 0)\n",
    "\n",
    "def p1(array):\n",
    "    return stats.scoreatpercentile(array, 10)\n",
    "\n",
    "def p2(array):\n",
    "    return stats.scoreatpercentile(array, 20)\n",
    "\n",
    "def p3(array):\n",
    "    return stats.scoreatpercentile(array, 30)\n",
    "\n",
    "def p4(array):\n",
    "    return stats.scoreatpercentile(array, 40)\n",
    "\n",
    "#中央値\n",
    "def p5(array):\n",
    "    return stats.scoreatpercentile(array, 50)\n",
    "\n",
    "def p6(array):\n",
    "    return stats.scoreatpercentile(array, 60)\n",
    "\n",
    "def p7(array):\n",
    "    return stats.scoreatpercentile(array, 70)\n",
    "\n",
    "def p8(array):\n",
    "    return stats.scoreatpercentile(array, 80)\n",
    "\n",
    "def p9(array):\n",
    "    return stats.scoreatpercentile(array, 90)\n",
    "\n",
    "#最大値\n",
    "def p10(array):\n",
    "    return stats.scoreatpercentile(array, 100)\n",
    "\n",
    "\n",
    "def return_rgb_features(function, r_array, g_array, b_array):\n",
    "    return [function(r_array), function(g_array), function(b_array), function(r_array+g_array), function(g_array+b_array), function(b_array+r_array), function(r_array+g_array+b_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#R,G,B,RG,GB,BR,RGBのヒストグラムの代表値を取得(直接フォルダから)\n",
    "def ft_extract_histogram_from_path(resize=100, imgpath= basepath +\"train/train\", num_of_img=len(y)):\n",
    "    \"\"\"\n",
    "    input: array with shape(row,col,3)\n",
    "    output: array with shape(row,91)\n",
    "    \"\"\"\n",
    "    img_feature_cols=[\"R_mean\", \"G_mean\", \"B_mean\", \"RG_mean\", \"GB_mean\", \"BR_mean\",\"RGB_mean\",\n",
    "             \"R_median\", \"G_median\", \"B_median\", \"RG_median\", \"GB_median\", \"BR_median\",\"RGB_median\",\n",
    "             \"R_std\", \"G_std\", \"B_std\", \"RG_std\", \"GB_std\", \"BR_std\",\"RGB_std\",\n",
    "             \"R_max\", \"G_max\", \"B_max\", \"RG_max\", \"GB_max\", \"BR_max\",\"RGB_max\",\n",
    "             \"R_min\", \"G_min\", \"B_min\", \"RG_min\", \"GB_min\", \"BR_min\",\"RGB_min\",\n",
    "             \"R_1\", \"G_1\", \"B_1\", \"RG_1\", \"GB_1\", \"BR_1\",\"RGB_1\",\n",
    "             \"R_2\", \"G_2\", \"B_2\", \"RG_2\", \"GB_2\", \"BR_2\",\"RGB_2\",                  \n",
    "             \"R_3\", \"G_3\", \"B_3\", \"RG_3\", \"GB_3\", \"BR_3\",\"RGB_3\",    \n",
    "             \"R_4\", \"G_4\", \"B_4\", \"RG_4\", \"GB_4\", \"BR_4\",\"RGB_4\",\n",
    "             \"R_6\", \"G_6\", \"B_6\", \"RG_6\", \"GB_6\", \"BR_6\",\"RGB_6\",                  \n",
    "             \"R_7\", \"G_7\", \"B_7\", \"RG_7\", \"GB_7\", \"BR_7\",\"RGB_7\",  \n",
    "             \"R_8\", \"G_8\", \"B_8\", \"RG_8\", \"GB_8\", \"BR_8\",\"RGB_8\",\n",
    "             \"R_9\", \"G_9\", \"B_9\", \"RG_9\", \"GB_9\", \"BR_9\",\"RGB_9\"]\n",
    "    img_feature = pd.DataFrame(columns=img_feature_cols)\n",
    "\n",
    "    for fn in range(num_of_img):\n",
    "        img_array = cv2.resize(cv2.imread(imgpath+\"_%s.jpg\"%(fn)),(resize,resize))\n",
    "        r_array = []\n",
    "        g_array = []\n",
    "        b_array = []\n",
    "        for i in range(resize):\n",
    "            for j in range(resize):  \n",
    "                if sum(img_array[i,j]) != 765: #(255*3)\n",
    "                    r_array.append(img_array[i,j,0])\n",
    "                    g_array.append(img_array[i,j,1])\n",
    "                    b_array.append(img_array[i,j,2])\n",
    "\n",
    "        add = pd.DataFrame([return_rgb_features(np.mean, r_array=r_array, g_array=g_array, b_array = b_array) + \n",
    "                            return_rgb_features(np.median, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(np.std, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(np.max, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(np.min, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p1, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p2, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p3, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p4, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p6, r_array=r_array, g_array=g_array, b_array = b_array)+                        \n",
    "                            return_rgb_features(p7, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p8, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p9, r_array=r_array, g_array=g_array, b_array = b_array)]                       \n",
    "                           , columns=img_feature_cols)\n",
    "        img_feature = img_feature.append(add)\n",
    "    img_feature.index = range(len(img_feature))\n",
    "    return img_feature\n",
    "\n",
    "#すでにXとかを読み込んでる場合にヒストグラムを取得\n",
    "def ft_extract_histogram_from_object(X_array,index):\n",
    "    img_feature_cols=[\"R_mean\", \"G_mean\", \"B_mean\", \"RG_mean\", \"GB_mean\", \"BR_mean\",\"RGB_mean\",\n",
    "             \"R_median\", \"G_median\", \"B_median\", \"RG_median\", \"GB_median\", \"BR_median\",\"RGB_median\",\n",
    "             \"R_std\", \"G_std\", \"B_std\", \"RG_std\", \"GB_std\", \"BR_std\",\"RGB_std\",\n",
    "             \"R_max\", \"G_max\", \"B_max\", \"RG_max\", \"GB_max\", \"BR_max\",\"RGB_max\",\n",
    "             \"R_min\", \"G_min\", \"B_min\", \"RG_min\", \"GB_min\", \"BR_min\",\"RGB_min\",\n",
    "             \"R_1\", \"G_1\", \"B_1\", \"RG_1\", \"GB_1\", \"BR_1\",\"RGB_1\",\n",
    "             \"R_2\", \"G_2\", \"B_2\", \"RG_2\", \"GB_2\", \"BR_2\",\"RGB_2\",                  \n",
    "             \"R_3\", \"G_3\", \"B_3\", \"RG_3\", \"GB_3\", \"BR_3\",\"RGB_3\",    \n",
    "             \"R_4\", \"G_4\", \"B_4\", \"RG_4\", \"GB_4\", \"BR_4\",\"RGB_4\",\n",
    "             \"R_6\", \"G_6\", \"B_6\", \"RG_6\", \"GB_6\", \"BR_6\",\"RGB_6\",                  \n",
    "             \"R_7\", \"G_7\", \"B_7\", \"RG_7\", \"GB_7\", \"BR_7\",\"RGB_7\",  \n",
    "             \"R_8\", \"G_8\", \"B_8\", \"RG_8\", \"GB_8\", \"BR_8\",\"RGB_8\",\n",
    "             \"R_9\", \"G_9\", \"B_9\", \"RG_9\", \"GB_9\", \"BR_9\",\"RGB_9\"]\n",
    "    img_feature = pd.DataFrame(columns=img_feature_cols)\n",
    "    \n",
    "    for img_array in X_array:\n",
    "        r_array = []\n",
    "        g_array = []\n",
    "        b_array = []\n",
    "        for i in range(len(img_array)):\n",
    "            for j in range(len(img_array[i])):  \n",
    "                if sum(img_array[i,j]) != 765: #(255*3)\n",
    "                    r_array.append(img_array[i,j,0])\n",
    "                    g_array.append(img_array[i,j,1])\n",
    "                    b_array.append(img_array[i,j,2])\n",
    "\n",
    "        add = pd.DataFrame([return_rgb_features(np.mean, r_array=r_array, g_array=g_array, b_array = b_array) + \n",
    "                            return_rgb_features(np.median, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(np.std, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(np.max, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(np.min, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p1, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p2, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p3, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p4, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p6, r_array=r_array, g_array=g_array, b_array = b_array)+                        \n",
    "                            return_rgb_features(p7, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p8, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                            return_rgb_features(p9, r_array=r_array, g_array=g_array, b_array = b_array)]                       \n",
    "                           , columns=img_feature_cols)\n",
    "        img_feature = img_feature.append(add)\n",
    "    img_feature.index = index\n",
    "    return img_feature\n",
    "\n",
    "\n",
    "#画像を分割して、R,G,B,RG,GB,BR,RGBのヒストグラムの代表値を取得\n",
    "def ft_extract_partial_histogram(X_array=X_train,size=5,stride=5,index=y_train.index):\n",
    "    c_max = X_array.shape[1]\n",
    "    r_max = X_array.shape[2]    \n",
    "    total_size = len(range(0,c_max-size+1,stride))*len(range(0,r_max-size+1,stride))\n",
    "    print(\"output size: (%s*%s)\"%(len(X_array),total_size*91))\n",
    "    \n",
    "    img_feature_cols = [\"f%s\"%(f) for f in range(total_size*91)]\n",
    "    img_feature = pd.DataFrame(columns=img_feature_cols)\n",
    "    \n",
    "    for xt in X_array:\n",
    "        add2 = [] #featureをまとめるやつ\n",
    "        for i in range(0,c_max-size+1,stride):\n",
    "            for j in range(0,r_max-size+1,stride):\n",
    "                if (i+size) > c_max: #上限を超えてしまう場合(多分ない)\n",
    "                    over = (i+size)-c_max\n",
    "                    i = i-over\n",
    "                if (j+size) > r_max: #上限を超えてしまう場合(多分ない)\n",
    "                    over = (j+size)-r_max\n",
    "                    j = j-over     \n",
    "                    \n",
    "                #size*sizeの画像のヒストグラムを取得する\n",
    "                img_array = xt[i:i+size,j:j+size]\n",
    "                r_array = []\n",
    "                g_array = []\n",
    "                b_array = []\n",
    "                \n",
    "                for s in range(len(img_array)):\n",
    "                    for t in range(len(img_array[s])):  \n",
    "                        if sum(img_array[s,t]) != 765: #背景の白は飛ばす\n",
    "                            r_array.append(img_array[s,t,0])\n",
    "                            g_array.append(img_array[s,t,1])\n",
    "                            b_array.append(img_array[s,t,2])\n",
    "                            \n",
    "                #画像がすべて白だった場合\n",
    "                if r_array == []:\n",
    "                    r_array = g_array = b_array = list(np.zeros(91))\n",
    "                    \n",
    "                #個々のfeature\n",
    "                add = [return_rgb_features(np.mean, r_array=r_array, g_array=g_array, b_array = b_array) + \n",
    "                                    return_rgb_features(np.median, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(np.std, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(np.max, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(np.min, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(p1, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(p2, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(p3, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(p4, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(p6, r_array=r_array, g_array=g_array, b_array = b_array)+                        \n",
    "                                    return_rgb_features(p7, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(p8, r_array=r_array, g_array=g_array, b_array = b_array)+\n",
    "                                    return_rgb_features(p9, r_array=r_array, g_array=g_array, b_array = b_array)][0]   \n",
    "                #個々のfeatureを横にまとめる                \n",
    "                add2 = add2 + add.copy()\n",
    "    \n",
    "        add_append = pd.DataFrame([add2], columns=img_feature_cols)\n",
    "        img_feature = img_feature.append(add_append)\n",
    "    img_feature.index = index\n",
    "    return img_feature\n",
    "\n",
    "\n",
    "#rawピクセル reshapeをする\n",
    "def ft_raw(rgb_array, cnn, resize):\n",
    "    if cnn:\n",
    "        return rgb_array.reshape(len(rgb_array), resize, resize, 3)\n",
    "    else:\n",
    "        return rgb_array.reshape(len(rgb_array), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train_his = ft_extract_histogram_from_object(X_train,index=y_train.index)\n",
    "#X_test_his  = ft_extract_histogram_from_object(X_test, index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_his = ft_extract_partial_histogram(X_train,size=10,stride=10,index=y_train.index)\n",
    "X_test_his  = ft_extract_partial_histogram(X_test ,size=10,stride=10,index=y_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"metrics\">Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "#ROC_AUC\n",
    "#閾値を変化させた場合の真陽率, 偽陽率の関係(only binary class)\n",
    "#http://qiita.com/HirofumiYashima/items/3f089f266f0404122cc9\n",
    "def mt_auc(clf, X_train, y_train):\n",
    "    y_train_proba = clf.predict_proba(X_train)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, y_score=y_train_proba, pos_label=2)\n",
    "    return roc_auc_score(y_true=y_train, y_score=y_train_proba)\n",
    "\n",
    "#精度\n",
    "def mt_M_accuracy(y_true, y_pred):\n",
    "    M_acc = {}\n",
    "    for i in range(24):\n",
    "        y_id = y_pred[y_true==i] == i\n",
    "        M_acc[i]=np.mean(y_id)\n",
    "    return  np.mean([j for j in M_acc.values()]), M_acc\n",
    "\n",
    "#学習済み学習器で精度を出力\n",
    "def mt_show_accs(clf,keras,X_train, X_test, y_train, y_test):\n",
    "    if keras:\n",
    "        y_train_pred = clf.predict_classes(X_train)\n",
    "        y_test_pred  = clf.predict_classes(X_test) \n",
    "        \n",
    "    else:\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_test_pred  = clf.predict(X_test)        \n",
    "    print(\"<------- Train ------->\")    \n",
    "    print(classification_report(y_true=y_train, y_pred=y_train_pred))\n",
    "    print(\"<------- Test ------->\")    \n",
    "    print(classification_report(y_true=y_test,  y_pred=y_test_pred))    \n",
    "    #return y_train_pred, y_test_pred\n",
    "    train_accuracy, train_M_acc = mt_M_accuracy(y_true=y_train, y_pred=y_train_pred)\n",
    "    test_accuracy, test_M_acc  = mt_M_accuracy(y_true=y_test, y_pred=y_test_pred)\n",
    "    print(\"\\nTrain Accuracy: %s\\n Test Accuracy: %s\"%(train_accuracy, test_accuracy))    \n",
    "    return train_M_acc, test_M_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"classifiers\">Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier,IsolationForest\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import xgboost as xgb\n",
    "\n",
    "classifiers = [\n",
    "    IsolationForest(random_state=0),\n",
    "    ExtraTreesClassifier(),\n",
    "    SVC(class_weight=None),\n",
    "    SVC(class_weight='auto'),\n",
    "    LinearSVC(),\n",
    "    DecisionTreeClassifier( random_state=0 ),\n",
    "    RandomForestClassifier( random_state=0 ),\n",
    "    AdaBoostClassifier( random_state=0 ),\n",
    "    GradientBoostingClassifier( random_state=0 ),\n",
    "    KNeighborsClassifier(),\n",
    "    xgb.XGBClassifier(),\n",
    "    GaussianNB(),\n",
    "    MultinomialNB(), \n",
    "    BernoulliNB()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtc = xgb.XGBClassifier(n_estimators=1000)\n",
    "xtc.fit(X_train_his,y_train[\"category_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_result,test_result = mt_show_accs(xtc, \n",
    "                                        keras=False, \n",
    "                                        X_train=X_train_his, \n",
    "                                        X_test=X_test_his, \n",
    "                                        y_train=y_train[\"category_id\"].values, \n",
    "                                        y_test=y_test[\"category_id\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"integration\">Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#24個のモデルを統合\n",
    "def integrate24models():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"result\">Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
